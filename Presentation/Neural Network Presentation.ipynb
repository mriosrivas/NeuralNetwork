{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Neural Network Implementation\n",
    "In the following Jupyter Notebook an Integer Neural Network Implementation is presented. Neural Networks are commonly implemented using floating point arithmetic, but if we want to implement it into hardware, the use of integer arithmetic will be more convenient. When making an integer implementation many issues can arise, such as overflow, signed arithmetic, function implementations (i.e. sigmoid), etc. \n",
    "\n",
    "In this Jupyter Notebook the following hardware issues are covered:\n",
    "* Neural Network Architecture\n",
    "* Neurons on hardware\n",
    "* Forward pass\n",
    "* Cost function\n",
    "* Backward pass\n",
    "* Parameter update\n",
    "* How to test your design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "The Neural Network architecture developed here is a three layer network, with two hidden layers and one input and one output layer. This simple architecture is shown in the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](Images/nn_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when making Neural Networks on hardware training is performed on software, and inference is performed on hardware. There are many reasons why training is preferred to be done on software such as:\n",
    "* Backpropagation algorithm is easier to implement\n",
    "* Activation functions are easily performed on software\n",
    "* Regularization, such as L2, is simple to implement\n",
    "* Optimization algorithms, like Adam an RMSprop, are hard to develop on hardware\n",
    "\n",
    "In this work we propose to make both **training and inference on hardware**. In the training case, we propose to perform backpropagation and activation functions on hardware. Regularization and optimization algorithms are not performed, but the used of bit shifting can have similar effects as a regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons on Hardware\n",
    "\n",
    "If you look at the image, you can see that the number of connections a neuron has depends on the previous number of neurons on the previous layer. This situation is not that important when you deal with floating point data, but if you work with integer numbers, the number of connections is limited by the integer precision. \n",
    "\n",
    "To understand this, consider how we multiply two integer numbers, $x$ and $w$ of size $N$, then $wx$ would be of size $2N$. Now if you make a calculation like this $w_{1}x_{1}+w_{2}x_{2}+ \\cdots w_{k}x_{k}$ the size would be $\\log_{2}(k)+2N$.\n",
    "\n",
    "![Multiplication bit size](Images/bit_size_mult.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that if you have a single neuron, you can connect up to $k$ inputs to a neuron, and as trade-off you will add $\\log_{2}(k)$ bits. You have to be careful on how many inputs can be stacked together before having an overflow. \n",
    "Now if you consider a single neuron as a two step calculation such as:\n",
    "\n",
    "1. Calculation of output $z$\n",
    "```python\n",
    "z = w1*x1+w2*x2+...+wk*xk\n",
    "```\n",
    "2. Calculation of activation $a$\n",
    "```python\n",
    "a = sigmoid(z)\n",
    "```\n",
    "![Single neuron multiple connection](Images/single_neuron_multiple_connection.png)\n",
    "\n",
    "The activation function must be limited to a correct bit size. We can perform this task by shifting our $z$ value to the right $N$ times, which in turn makes our input activation values lower to handle. With this our activation function maps from $\\log_{2}(k)+N$ to $N$ bits.\n",
    "\n",
    "![Activation bit size](Images/bit_size_activ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we implement a single neuron on hardware?\n",
    "We don't implement a single neuron, we implement a layer of neurons and make use of the ideas previously presented. To do so, we will use *System Verilog*. SystemVerilog is a superset of *Verilog* language and is getting more widely adopted in industry. It is an IEEE standard and supports lot of enhancements from Verilog for design constructs. In addition SystemVerilog supports all features for Verification - including OOPs , assertions, coverage etc , thus making a single language suitable for both design and verification. If you know Verilog you will easily make the move to System Verilog.\n",
    "\n",
    "First we want to achieve the following calculation:\n",
    "\n",
    "```python\n",
    "# W weight matrix\n",
    "# X input vector\n",
    "# b bias vector\n",
    "# sigmoid and relu are activation functions\n",
    "Z = W*X+b\n",
    "if('sigmoid_activation'):\n",
    "    A=sigmoid(Z)\n",
    "elif('relu_activation'):\n",
    "    A=relu(Z)\n",
    "```\n",
    "As we already mention, we divided this problem into two parts:\n",
    "\n",
    "#### 1. Calculation of output $z$\n",
    "We further divide this problem into two parts:\n",
    "##### 1.1 Calculation of product $W\\cdot X$\n",
    " \n",
    "Remember when you multiply a matrix $W$ and a vector $X$ you can perform this calculation as a recursive MAC operation as follows:\n",
    "\n",
    "```python\n",
    "# M is row size\n",
    "# N is column size\n",
    "for i in M:\n",
    "    for j in N:\n",
    "        y[i][0] = y[i][0] + W[i][j]*X[j][0]\n",
    "\n",
    "```\n",
    "\n",
    "This can be achieve in System Verilog as:\n",
    "\n",
    "```c\n",
    "module mult #(parameter M, parameter N)\n",
    "    ...\n",
    "// B is a temporal matrix that stores vector-matrix products\n",
    "always_ff @(posedge clk) begin\n",
    "        for(i=0;i<=M-1;i=i+1)\n",
    "            for(j=0; j<=N-1; j=j+1)\n",
    "                B[i][j] = A[i][j]*x[j][0];  \n",
    "    end \n",
    "\n",
    "...\n",
    "        \n",
    "// y is a vector that sums all B values in the vertical axis\n",
    "genvar i, j;\n",
    "        generate\n",
    "            for(i=0;i<M;i=i+1) begin\n",
    "            assign y[i][0] = B[i][0] + B[i][1];\n",
    "                for(j=0; j<=N-3; j=j+1)\n",
    "                    always_ff @(posedge clk) begin\n",
    "                            y[i][j+1] <= y[i][j] + B[i][j+2];\n",
    "                        end\n",
    "           end     \n",
    "        endgenerate \n",
    "endmodule\n",
    "\n",
    "```\n",
    "\n",
    "One of the language most powerful statements is the `generate` statement because it allow us the create hardware designs in a recursive way as this example.\n",
    "\n",
    "![Multiplier](Images/multiplier.png)\n",
    "\n",
    "##### 1.2 Addition of $b$ to product $W \\cdot X$\n",
    "\n",
    "```python\n",
    "# M is row size\n",
    "for i in M:\n",
    "    Z[i][0] = Z[i][0] + b[i][0]\n",
    "\n",
    "```\n",
    "\n",
    "This can also be achieved using System Verilog as follows:\n",
    "\n",
    "```C\n",
    "module bias #(parameter M)\n",
    "    ...\n",
    "genvar i;\n",
    "     generate\n",
    "         for (i=0; i<M; i=i+1) begin\n",
    "             always_ff @(posedge clk) begin\n",
    "                if(!reset) a[i][0] <= 0;\n",
    "                else begin\n",
    "                    a[i][0]<=b[i][0] + z[i][0]; \n",
    "                end\n",
    "             end\n",
    "         end\n",
    "     endgenerate\n",
    "endmodule\n",
    "```\n",
    "\n",
    "\n",
    "![Bias](Images/bias.png)\n",
    "\n",
    "#### 2. Calculation of activation $a$\n",
    "For the activation function, two types were considered:\n",
    "##### 2.1 RELU Unit\n",
    "For this type of function we need to implement the following calculation:\n",
    "```python\n",
    "A = maximum(0, Z)\n",
    "```\n",
    "Since this is just a clipping of values lower than zero, this can be done by implementing this:\n",
    "```c\n",
    "module relu #(parameter M = 5)\n",
    "...\n",
    "    \n",
    " genvar i;\n",
    "     generate\n",
    "         for (i=0; i<M; i=i+1) begin\n",
    "             always_ff @(posedge clk) begin\n",
    "                if(!reset) a[i][0] <= 0;\n",
    "                else begin\n",
    "                    if(z[i][0]<0) a[i][0]<= 0;\n",
    "                    else a[i][0]<=z[i][0]; \n",
    "                end\n",
    "             end\n",
    "         end\n",
    "     endgenerate\n",
    "    \n",
    "endmodule\n",
    "```\n",
    "\n",
    "![Relu](Images/relu.png)\n",
    "##### 2.2 Sigmoid Unit\n",
    "For this type of unit, a table implementation was performed. On real numbers Relu performs this calculation:\n",
    "$$ \\sigma(z)=\\frac{1}{1-e^{-z}}$$ \n",
    "As discussed before, our input values are of size $\\log_{2}(k)+N$, which means that when mapping we have to be careful to cover this range of inputs. The actual table implementation maps $\\sigma(z)$ to $N$ bits by calculating a table for all possible values $z$ like this:\n",
    "\n",
    "$$ \\sigma(z)=\\frac{1}{1-e^{-z/(\\log_{2}(k)+N)}}$$ \n",
    "\n",
    "Table size can be reduced if redundancy was removed. For this case a 96% reduction in size was achieved if using *if-else* clauses to create this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "For a Neural Network, the implementation of a forward pass is just the calculation of this equation:\n",
    "\n",
    "$$ A = \\sigma(WX+b)$$\n",
    "\n",
    "and pass vector $A$ to the next layer of neurons.\n",
    "\n",
    "So an implementation might look like this:\n",
    "\n",
    "```python\n",
    "def forward(W,X,b):\n",
    "    return activation(W*X + b)\n",
    "\n",
    "A1 = forward(W1,X1,b1)\n",
    "A2 = forward(W2,A1,b2)\n",
    "AL = forward(W3,A2,b3)\n",
    "\n",
    "```\n",
    "\n",
    "A hardware implementation will make use of the previous hardware blocks as follows:\n",
    "\n",
    "```c\n",
    "// M number of neurons\n",
    "// N input size or previous number of neurons\n",
    "\n",
    "module forward_neurons #(parameter M, parameter N) \n",
    "    ...    \n",
    "mult #(M, N) multiplier(.clk(clk), .A(A), .x(x), .reset(reset), .b(Ax));\n",
    "bias #(M) add_bias(.clk(clk), .z(Ax), .b(b), .reset(reset), .a(z)); \n",
    "relu #(M) activation(.clk(clk), .z(z), .reset(reset), .a(a));  \n",
    "    \n",
    "endmodule \n",
    "```\n",
    "\n",
    "And then make an instance of those modules\n",
    "```c\n",
    "module forward_net(\n",
    "    ...\n",
    "    // Forward Neuron Network\n",
    "    forward_neurons #(L2, L1) layer_1(clk, W1, a1, b1, reset, a2);\n",
    "    forward_neurons #(L3, L2) layer_2(clk, W2, a2, b2, reset, a3);\n",
    "    forward_neurons #(L4, L3) layer_3(clk, W3, a3, b3, reset, a4);\n",
    "    \n",
    "    ...\n",
    " endmodule\n",
    " ```\n",
    " \n",
    " ![Forward](Images/forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Cost Function\n",
    "To calculate our cost, we will use the cross entropy loss function defined as:\n",
    "$$L = -y\\log{\\hat{y}}-(1-y)\\log{(1-\\hat{y})}$$\n",
    "\n",
    "As implemented for the sigmoid function, we also applied a logarithm table, but in this case there is a one on one mapping, therefore no table reduction was achieved.\n",
    "\n",
    "Cost, can be calculated on two different ways:\n",
    "1. Using stochastic gradient descent, SGD\n",
    "2. Gradient descent, GD\n",
    "\n",
    "If using SGD cost is calculated on every input for every epoch, on GD cost is calculated on average for each epoch. In terms of hardware design SGD is easier to implement, but raises convergence issues.\n",
    "\n",
    "For the gradient of the loss, we also create a table that performs this calculation:\n",
    "$$\\frac{dL}{dA} = -\\frac{y}{\\hat{y}}+\\frac{(1-y)}{(1-\\hat{y})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "Backward pass works by using gradient descent to solve the problem of reducing the cost of our network. When implementing backward propagation, two main blocks can be defined:\n",
    "1. Activation backward\n",
    "2. Linear backward\n",
    "\n",
    "as shown in the image.\n",
    "\n",
    "![Backward](Images/backward.png)\n",
    "\n",
    "### 1. Activation backward\n",
    "Activation backward depends on the type of activation function and calculates the derivative of the cost with respect of its output. \n",
    "\n",
    "#### 1.1 Relu activation backward\n",
    "The implementation of relu backward is very simple, and can be implemented as:\n",
    "```python\n",
    "    # dZ is the derivative of loss with respect to Z\n",
    "    # dA is the derivative of loss with respect to A\n",
    "\n",
    "    if(Z>0):\n",
    "        dZ = 1*dA\n",
    "    else:\n",
    "        dZ = 0*dA\n",
    "```\n",
    "\n",
    "#### 1.2 Sigmoid activation backward\n",
    "The implementation of sigmoid backward is more complex, and it introduces the calculation of the following:\n",
    "$$\\frac{dL}{dZ}=\\frac{dL}{dA}s(1-s)$$\n",
    "\n",
    "```python\n",
    "# dZ is the derivative of loss with respect to Z\n",
    "# dA is the derivative of loss with respect to A\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "```\n",
    "\n",
    "Hardware implementations can be done reusing the sigmoid table and taking care of the bit size as it increases due to the three multiplications involved.\n",
    "\n",
    "### 2. Linear backward\n",
    "Linear backward calculation can be performed in software as follows:\n",
    "\n",
    "```python\n",
    "# dW is the derivative of loss with respect to W\n",
    "# db is the derivative of loss with respect to b\n",
    "# dA_prev is the derivative of loss with respect to A_(L-1)\n",
    "\n",
    "dW = dZ*transpose(A)\n",
    "db = dZ\n",
    "dA_prev = transpose(W)*dZ\n",
    "```\n",
    "\n",
    " ![Backward](Images/backdrop_hw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter update\n",
    "Parameter update is implemented by doing the following calculations:\n",
    "\n",
    "$$W = W - \\alpha \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\alpha \\frac{dL}{db}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate of our network. If our learning rate is to big the network might not converge, and if the learning rate is to small it might learn very slowly.\n",
    "\n",
    "To make this implementation in hardware, our learning rate is just a number that shifts $\\frac{dL}{dW}$ or $\\frac{dL}{db}$ to the right, this in turn reduces the possible learning rates that can be performed but gives us a very simple hardware.\n",
    "\n",
    "```c\n",
    "// y is the parameter to update\n",
    "// dx is the gradient\n",
    "// learning_rate is a number\n",
    "assign y[i][j] = x[i][j] + (dx[i][j]>>learning_rate);   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "When you develop complex hardware designs there are many issues that can go wrong, therefore a good methodology is always good. Here are some guidelines than can help you:\n",
    "\n",
    "1. Write your ideas in paper and come with an architecture.\n",
    "2. Simulate your project in software, and solve the simple case.\n",
    "3. Try to make your software simulation into hardware.\n",
    "4. Make good software practices when writing software and hardware code.\n",
    "5. If you can make scripts, do it. They will save you time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
