import numpy as np


def sigmoid(Z, table, extra_bits=4, sigmoid_decimal_bits=12):
    """
    Implements the sigmoid activation in numpy

    :param extra_bits:
    :param Z: numpy array of any shape
    :param table: sigmoid activation table generated by function sigmoid_table()
    :param sigmoid_decimal_bits: number of bits used for decimal values
    :return: A -- output of sigmoid(z), same shape as Z
            cache -- returns Z as well, useful during backpropagation
    """

    A = get_sigmoid_table_data(Z, table, extra_bits, sigmoid_decimal_bits)
    cache = Z

    return A, cache


def get_sigmoid_table_data(Z, table, extra_bits=4, sigmoid_decimal_bits=12):
    """
    Table lookup of sigmoid data.

    If input data is positive: out = table[input]
    If input data is negative: out = -table[input] + 2**sigmoid_decimal_bits

    :param Z: input to calculate sigmoid activation function
    :param table: sigmoid activation integer table generated by function sigmoid_table()
    :param extra_bits: bits used to increment range of input values
    :param sigmoid_decimal_bits: number of bits used for decimal values
    :return: A -- output integer activation function
    """

    A = np.zeros(Z.shape)

    # Wrap Z values between -2 ** (EXTRA_BITS + sigmoid_decimal_bits) and 2 ** (EXTRA_BITS + sigmoid_decimal_bits) - 1
    Z[Z > 2 ** (extra_bits + sigmoid_decimal_bits) - 1] = 2 ** (extra_bits + sigmoid_decimal_bits) - 1
    Z[Z < -2 ** (extra_bits + sigmoid_decimal_bits)] = -2 ** (extra_bits + sigmoid_decimal_bits) - 1

    A[(Z < 0)] = -table[-Z[(Z < 0)]] + 2 ** sigmoid_decimal_bits
    A[(Z >= 0)] = table[Z[(Z >= 0)]]
    # try:
    #     A[(Z < 0)] = -table[-Z[(Z < 0)]] + 2 ** sigmoid_decimal_bits
    # except:
    #     print(Z)
    # # print('sigmoid')
    # # print('{} + {}'.format(-table[-Z[(Z<0)]], 2**sigmoid_decimal_bits))
    # # print('sigmoid_decimal_bits = {}'.format(sigmoid_decimal_bits))
    # try:
    #     A[(Z >= 0)] = table[Z[(Z >= 0)]]
    # except:
    #     print(Z)

    return A


def relu(Z):
    """
    Relu function calculation of input Z

    :param Z: output of the linear layer, of any shape
    :return: A -- Post-activation parameter, of the same shape as Z
            cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently
    """

    A = np.maximum(0, Z)
    assert (A.shape == Z.shape)
    cache = Z
    return A, cache


def sigmoid_backward(dA, cache, table, sigmoid_decimal_bits=12, precision='int32'):
    """

    :param dA: post-activation gradient, of any shape
    :param cache: 'Z' where we store for computing backward propagation efficiently
    :param table: sigmoid activation integer table generated by function sigmoid_table()
    :param sigmoid_decimal_bits: number of bits used for decimal values
    :param precision: integer PRECISION of implementation
    :return: dZ -- integer gradient of the cost with respect to Z with sigmoid_decimal_bits PRECISION
    """

    Z = cache

    # dZ is calculated by tables and retrieves the following operations:
    # s = 1/(1+np.exp(-Z))
    # dZ = dA * s * (1-s)
    dZ = dA * get_sigmoid_gradient_from_table(Z, table)

    assert (dZ.shape == Z.shape)
    return (dZ >> sigmoid_decimal_bits).astype(precision)


def get_sigmoid_gradient_from_table(Z, table, sigmoid_decimal_bits=12, precision='int32'):
    """
    Function that performs the following calculation s * (1-s) where s = 1/(1+np.exp(-Z)). The value of s
    is retrieved from table.

    :param Z: output of the linear layer, of any shape
    :param table: sigmoid activation integer table generated by function sigmoid_table()
    :param precision: integer PRECISION of implementation
    :param sigmoid_decimal_bits: number of bits used for decimal values
    :return: calculation s * (1-s)
    """

    s, _ = sigmoid(Z, table)
    s = s.astype(precision)
    # return ((s * (2**sigmoid_decimal_bits - s)).astype(precision)) >> sigmoid_decimal_bits
    return ((s * (2**sigmoid_decimal_bits - s)) >> sigmoid_decimal_bits).astype(precision)


def relu_backward(dA, cache):
    """
    Function that performs the backward propagation for a single relu unit.

    :param dA: post-activation gradient, of any shape
    :param cache: 'Z' where we store for computing backward propagation efficiently
    :return: dZ -- Gradient of the cost with respect to Z
    """

    Z = cache
    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.
    dZ[Z <= 0] = 0
    assert (dZ.shape == Z.shape)
    return dZ
