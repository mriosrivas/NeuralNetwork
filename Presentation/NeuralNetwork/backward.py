import numpy as np
from activations import *


def linear_backward(dZ, cache, sigmoid_decimal_bits=12, precision='int32'):
    """
    Function that implements the linear portion of backward propagation for a single layer (layer l)

    :param dZ: gradient of the cost with respect to the linear output (of current layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :param sigmoid_decimal_bits: number of bits used for decimal values
    :param precision: integer PRECISION of implementation
    :return: dA_prev -- gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
            dW -- gradient of the cost with respect to W (current layer l), same shape as W
            db -- gradient of the cost with respect to b (current layer l), same shape as b
    """

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = (np.dot(dZ, A_prev.T) / m).astype(precision)
    dW = dW >> sigmoid_decimal_bits

    db = (np.sum(dZ, axis=1, keepdims=True) / m).astype(precision)

    dA_prev = (np.dot(W.T, dZ)).astype(precision)
    dA_prev = dA_prev >> sigmoid_decimal_bits

    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation, sigmoid_decimal_bits=12, precision='int32', **kwargs):
    """
    Function that implements the backward propagation for the LINEAR->ACTIVATION layer.
    :param dA: post-activation gradient for current layer l
    :param cache: tuple of values (linear_cache, activation_cache) we store for computing backward propagation
            efficiently
    :param activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    :param sigmoid_decimal_bits:
    :param precision: integer PRECISION of implementation
    :param kwargs: "table" sigmoid activation table generated by function sigmoid_table
    :return: dA_prev -- gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
            dW -- gradient of the cost with respect to W (current layer l), same shape as W
            db -- gradient of the cost with respect to b (current layer l), same shape as b
    """

    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache, sigmoid_decimal_bits, precision)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache, table=kwargs['table'],
                              sigmoid_decimal_bits=sigmoid_decimal_bits, precision=precision)
        dA_prev, dW, db = linear_backward(dZ, linear_cache, sigmoid_decimal_bits, precision)

    return dA_prev, dW, db
